{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"normalized_names.txt\") as f:\n",
    "    text = [line.lower().strip('\\n ') for line in f]\n",
    "    \n",
    "strain_names = []\n",
    "for t in text:\n",
    "    if ',' in t:\n",
    "        strain_names.extend(t.split(','))\n",
    "    else:\n",
    "        strain_names.append(t)\n",
    "names = [[\"<BOS>\"] + list(name) for name in strain_names]\n",
    "targets = [list(name) + [\"<EOS>\"] for name in strain_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_chars = {char for name in names for char in name}\n",
    "uniq_chars.update({'<EOS>'})\n",
    "char2idx = {char: i+1 for i, char in enumerate(uniq_chars)}\n",
    "char2idx['<MASK>'] = 0\n",
    "\n",
    "idx2char = {i: char for char, i in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = names[:1000]\n",
    "train_targets = targets[:1000]\n",
    "test_names = names[1000:]\n",
    "test_targets = targets[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<BOS>',\n",
       "  'g',\n",
       "  '1',\n",
       "  '3',\n",
       "  ' ',\n",
       "  's',\n",
       "  'u',\n",
       "  'p',\n",
       "  'e',\n",
       "  'r',\n",
       "  ' ',\n",
       "  's',\n",
       "  'i',\n",
       "  'l',\n",
       "  'v',\n",
       "  'e',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'a',\n",
       "  'z',\n",
       "  'e'],\n",
       " ['<BOS>', 'c', 'h', 'e', 'r', 'r', 'y', ' ', 'o', 'g', ' ', 'b', 'x'],\n",
       " ['<BOS>',\n",
       "  'm',\n",
       "  'e',\n",
       "  'n',\n",
       "  'd',\n",
       "  'o',\n",
       "  'c',\n",
       "  'i',\n",
       "  'n',\n",
       "  'o',\n",
       "  ' ',\n",
       "  'p',\n",
       "  'u',\n",
       "  'r',\n",
       "  'p',\n",
       "  'l',\n",
       "  'e'],\n",
       " ['<BOS>',\n",
       "  'l',\n",
       "  'o',\n",
       "  'o',\n",
       "  'm',\n",
       "  'p',\n",
       "  'a',\n",
       "  \"'\",\n",
       "  's',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'o',\n",
       "  'l',\n",
       "  'u',\n",
       "  'm',\n",
       "  'b',\n",
       "  'i',\n",
       "  'a',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'h',\n",
       "  'e',\n",
       "  'm',\n",
       "  'd',\n",
       "  'a',\n",
       "  'w',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'd',\n",
       "  ' ',\n",
       "  'f',\n",
       "  '2'],\n",
       " ['<BOS>', 'k', 'e', 'n', \"'\", 's', ' ', 'k', 'u', 's', 'h'],\n",
       " ['<BOS>', 's', 'p', 'i', 'c', 'y', ' ', 'c', 'b', 'd'],\n",
       " ['<BOS>', 's', 'p', 'a', 'c', 'e', ' ', 'k', 'u', 's', 'h'],\n",
       " ['<BOS>', 'k', 'a', 'r', 'm', 'd', 'o', 'w', 'n'],\n",
       " ['<BOS>',\n",
       "  'd',\n",
       "  'o',\n",
       "  'u',\n",
       "  'b',\n",
       "  'l',\n",
       "  'e',\n",
       "  ' ',\n",
       "  's',\n",
       "  't',\n",
       "  'u',\n",
       "  'f',\n",
       "  'f',\n",
       "  'e',\n",
       "  'd'],\n",
       " ['<BOS>', 'f', 'i', 'r', 'e', ' ', 'o', 'g', ' ', 'k', 'u', 's', 'h']]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = len(max(train_names, key=len))\n",
    "X_train = torch.zeros((len(train_names), max_seq_length)).long()\n",
    "for i, strain in enumerate(train_names):\n",
    "    for j in range(max_seq_length):\n",
    "        if j < len(strain):\n",
    "            X_train[i][j] = char2idx[strain[j]]\n",
    "        else:\n",
    "            X_train[i][j] = char2idx['<MASK>']\n",
    "\n",
    "max_seq_length = len(max(train_targets, key=len))\n",
    "y_train = torch.zeros((len(train_targets), max_seq_length)).long()\n",
    "for i, strain in enumerate(train_targets):\n",
    "    for j in range(max_seq_length):\n",
    "        if j < len(strain):\n",
    "            y_train[i][j] = char2idx[strain[j]]\n",
    "        else:\n",
    "            y_train[i][j] = char2idx['<MASK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = len(max(test_names, key=len))\n",
    "X_val = torch.zeros((len(test_names), max_seq_length)).long()\n",
    "for i, strain in enumerate(test_names):\n",
    "    for j in range(max_seq_length):\n",
    "        if j < len(strain):\n",
    "            X_val[i][j] = char2idx[strain[j]]\n",
    "        else:\n",
    "            X_val[i][j] = char2idx['<MASK>']\n",
    "\n",
    "max_seq_length = len(max(test_targets, key=len))\n",
    "y_val = torch.zeros((len(test_targets), max_seq_length)).long()\n",
    "for i, strain in enumerate(test_targets):\n",
    "    for j in range(max_seq_length):\n",
    "        if j < len(strain):\n",
    "            y_val[i][j] = char2idx[strain[j]]\n",
    "        else:\n",
    "            y_val[i][j] = char2idx['<MASK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(X, y, batch_size=25):\n",
    "    i = 0\n",
    "    while i < X.shape[0]:\n",
    "        xs = X[i:i+batch_size]\n",
    "        ys = y[i:i+batch_size]\n",
    "        i += batch_size\n",
    "        yield xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_samples(samples):\n",
    "    samples = samples.argmax(2)\n",
    "    all_strain_names = []\n",
    "    for sample in range(samples.shape[0]):\n",
    "        strain_name = \"\"\n",
    "        for time_step in range(samples.shape[1]):\n",
    "            char_idx = samples[sample, time_step].item()\n",
    "            char = idx2char[char_idx]\n",
    "            if char in ('<BOS>', '<EOS>', '<MASK>'):\n",
    "                continue\n",
    "            else:\n",
    "                strain_name += char\n",
    "        all_strain_names.append(strain_name)\n",
    "    return all_strain_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(y_pred, y_true):\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return torch.nn.functional.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(learning_rate, model_state_file):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': model_state_file}\n",
    "\n",
    "\n",
    "def update_train_state(early_stopping_criteria, model, train_state):\n",
    "    \"\"\"Handle the training state updates\"\"\"\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # If loss worsened\n",
    "        if loss_t >= loss_tm1:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "                \n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= early_stopping_criteria\n",
    "\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrainNameModel(torch.nn.Module):\n",
    "    def __init__(self, char2idx, embed_size=100, lstm_size=128):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(len(char2idx), embed_size, padding_idx=0)\n",
    "        self.lstm = torch.nn.LSTM(embed_size, lstm_size)\n",
    "        self.fc = torch.nn.Linear(lstm_size, len(char2idx))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out, _ = self.lstm(self.embed(X))\n",
    "        batch_size, seq_size, feat_size = out.shape\n",
    "        \n",
    "        out = out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "        out = self.fc(torch.nn.functional.dropout(out, p=0.75))\n",
    "        \n",
    "        # out = torch.nn.functional.softmax(out, dim=1)\n",
    "        \n",
    "        new_feat_size = out.shape[-1]\n",
    "        out = out.view(batch_size, seq_size, new_feat_size)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae4cae34d1b4a8ba8984cc6030e7390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training routine', max=1000, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f686ad66ce0c489892e9af0ed9507687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=train', max=251, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c50176a23244f18c3d55294e1c64a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=val', max=75, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting loop\n"
     ]
    }
   ],
   "source": [
    "mask_index = char2idx['<MASK>']\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.00001\n",
    "model_save_path = \"model.pth\"\n",
    "batch_size = 4\n",
    "early_stopping_criteria = 100\n",
    "\n",
    "model = StrainNameModel(char2idx)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                       mode='min', factor=0.3,\n",
    "                                                       patience=1)\n",
    "train_state = make_train_state(learning_rate, model_save_path)\n",
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total= 1 + (X_train.shape[0] // batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total= 1 + (X_val.shape[0] // batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_index, (X, y) in enumerate(batch_iter(X_train, y_train, batch_size)):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------    \n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = model(X)\n",
    "            # step 3. compute the loss\n",
    "            loss = sequence_loss(y_pred, y, mask_index)\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # -----------------------------------------\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, y, mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        model.eval()\n",
    "\n",
    "        for batch_index, (X, y)  in enumerate(batch_iter(X_val, y_val, batch_size)):\n",
    "            # compute the output\n",
    "            y_pred = model(X)\n",
    "            # step 3. compute the loss\n",
    "            loss = sequence_loss(y_pred, y, mask_index)\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, y, mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # Update bar\n",
    "            val_bar.set_postfix(loss=running_loss, \n",
    "                                acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(early_stopping_criteria, \n",
    "                                         model, \n",
    "                                         train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "        \n",
    "        # move model to cpu for sampling\n",
    "        sampled_strains = random.sample(decode_samples(y_pred), 3)\n",
    "        epoch_bar.set_postfix(sample1=sampled_strains[0], \n",
    "                              sample2=sampled_strains[1])\n",
    "        \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'r', 'i', 'p', 'l', 'e', ' ', 'x', '<EOS>']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2char[i.item()] for i in y[0] if i != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-9c6c1270c8f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecode_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-195-7a100665a738>\u001b[0m in \u001b[0;36mdecode_samples\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mall_strain_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstrain_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "decode_samples(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s  eeea   sa  ',\n",
       " 'sae a ase e eeeee',\n",
       " 'seeeeee s  ',\n",
       " 's  e aeaaeeeeeeeee eeeeee',\n",
       " 's e     sees a ee',\n",
       " 's   e s a  ee saea',\n",
       " 's e e   e a',\n",
       " 'sa     ees ee',\n",
       " 'sse esa  as e',\n",
       " 'see  eae e ']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(decode_samples(y_pred), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-a8d168756bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
